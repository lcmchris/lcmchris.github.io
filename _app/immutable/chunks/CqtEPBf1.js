import{f as r,a as l}from"./NeLHWg22.js";import"./BASVQn1p.js";import{C as e,D as c,F as d,v as h}from"./BBWcagcT.js";import{s as t}from"./DqS_7iSC.js";const m=""+new URL("../assets/vcc_pds_dominates_scoring.CJqLgBXl.png",import.meta.url).href,p=""+new URL("../assets/wandb_train_loss.CNmHbZsI.png",import.meta.url).href,u=""+new URL("../assets/wandb_val_loss.DfhkaVG3.png",import.meta.url).href,g={title:"A little foray into the virtual cell challenge.",tags:"Projects",first:"2025-11-27T00:00:00.000Z",last:"2025-11-27T00:00:00.000Z",uuid:"835a789f-0d91-4202-9c10-108e70299f0b"},{title:S,tags:T,first:D,last:P,uuid:_}=g;var f=r(`<p>The virtual cell challenge is a brand new challenge released by the <a href="https://arcinstitute.org/" rel="nofollow">Arc Institute</a> to help progress single cell predictions against perturbations. This research is crucial into our understanding of how cells behave under perturbations and allows us to create novel cells. It comes alongside their own release of <a href="https://github.com/ArcInstitute/state" rel="nofollow">State</a>, a model to do just this.</p> <blockquote><p>Understanding, predicting, and ultimately programming how cells respond to internal cues and external stimuli is a fundamental challenge in biology.</p></blockquote> <p>From
The data is split into 3. Training with 150 perturbations, Validation set with 50 perturbations and the final test set with 50 perturbations.</p> <h3>Timeline</h3> <p>The absence to the</p> <h3>The PDS spiral</h3> <p>Perturbation Discrimination Score is one of the core evaluation metrics and as it turns out, became almost the sole decider of the whole competition. This was due to a few factors.</p> <h4>Scoring is skewed towards PDS</h4> <p>The 3 evaluation metrics are Mean Absolute Error, Differential Expression Score (DES) and Perturbation Discrimination Score (PDS)</p> <p>Metrics are measured based on baseline value. The baselines are created using the aggregate cell-mean change.
The baseline ended up being 0.027 for MAE, 0.106 for DES and 0.514 for PDS.</p> <img width="" alt="Graphs by Fleetwood in discord https://discord.com/channels/1385383995704999947/1389397364401373335/1424753215131357286"/> Because DES and PDS are then scaled so that 1 is perfect while MAE is scaled so that 0 is perfect. This meant the gradient of the points value is vastly different. A single point increase in PDS amounts to much more than either DES or MAE. The added fact that any score under the baseline is cropped to zero, it meant MAE is almost useless (regardless of what the organisers are trying to convince us). <blockquote><p>We made a design choice this year not to penalize models that perform worse than the baseline on any one metric because they may still outperform on other metrics. We understand there is a downside to this approach: as the current leaderboard shows, some top-performing models are not doing well at all on MAE. But that does not mean MAE can’t be a differentiating factor, if some Challengers find a way to do better on that metric. <a href="https://discord.com/channels/1385383995704999947/1423038641764175882/1423746443583291516" rel="nofollow">https://discord.com/channels/1385383995704999947/1423038641764175882/1423746443583291516</a></p></blockquote> <p>Note PDS also already caused headache in the middle-end stage of the competition where a small code inaccuracy of an misused <code>abs</code> operator caused the metric to consider only absolute sizes of differences and not the direction. This was correctly changed by the organiser. <a href="https://discord.com/channels/1385383995704999947/1387880200880132359/1423037528046440652" rel="nofollow">https://discord.com/channels/1385383995704999947/1387880200880132359/1423037528046440652</a></p> <h4>Pseudo-bulking</h4> <p>PDS is calculated on the pseudo-bulk level. This means that all submitted cells of the same cell lines are grouped and average before it goes into the PDS calculation. This in turn makes the single-cell nature of the expression profiles a distraction for this metric. DES does look at single cell, but</p> <h3>Gaming the leaderboard</h3> <p>The validation and test set’s non-targeting cells are not released.</p> <h3>My small attempt</h3> <p>My attempt is to leverage a simple encoder-only transformer based on the following:</p> <p><code>Measured Gene ESM embedding</code> + <code>Perturbed Gene ESM embedding</code> + <code>Original control value random</code> => <code>Predicted value</code></p> <p>I never managed to get a proper validation workflow working with the validation set (which is a persistent problem amongst other participants as the control cells are different in the test sets). Hence as I was running out of time, I ended up only training via the loss function.</p> <img width="600" alt="Wandb Train loss"/> <img width="600" alt="Wandb Val loss"/> <p>On inference, I also pick random control cells from the original set. The charts look nice but the performance was not!</p> <h3>Aftermath</h3>`,1);function E(n){var a=f(),o=e(c(a),20),s=e(o,22),i=e(s,2);h(4),d(()=>{t(o,"src",m),t(s,"src",p),t(i,"src",u)}),l(n,a)}export{E as default,g as metadata};
