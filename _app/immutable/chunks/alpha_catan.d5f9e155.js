import{s as Oe,n as Wt}from"./scheduler.8b75dc5a.js";import{S as Se,i as Fe,g as i,s,h as o,x as a,c as r,a as l,f as n}from"./index.cd4b7fee.js";function Ue(Be){let h,Dt="Building an AI",tt,u,Jt='For the better part of the last year, I’ve been working on a little project I called <a href="https://github.com/lcmchris/alpha-catan" rel="nofollow"><code>alpha-catan</code></a>. I wanted to work on something just fun and interesting. Nothing related to business or anything that is even (remotely) useful. Just pure unadulterated fun. The project I ended up working on is trying to build and AI that plays Catan.',et,p,Kt="Now, Settlers of Catan is a pretty famous board game which I have personally despised for ages. Apart from the terrible experiences of fights during Catan, I strongly believe there are 2 fundamental flaws of the game.",lt,d,Yt="<li><p>Economy acceleration through luck: There is a strong economy acceleration effect. Good rolls give you resources which gives you more settlements… giving you more chances for good rolls. The <code>skill</code> is where to put your settlements. This however creates a situation where in some games you become merely an observer waiting for the winner to slowly play out the game.</p></li> <li><p>Trading: Many will argue the main game is the trading. From my experience, it is merely a tool to abuse new players. There is no reason really to trade as it is a zero-sum game. You would only accept a trade if you would increase your win% and a trade cannot do it for both parties of the trade. The only argument is if coming 2nd vs 3rd matters (which in general party games, it doesn’t).</p></li>",nt,m,Qt=`These flaws are also why I believe a relatively simple model can beat the majority of players. If the only action that matters is mostly the first one, then it should be relatively simple to solve!
Armed with the <code>Hands on Machine learning book</code> and my hatred of Catan, I started working on building the model and the controller that goes with it.`,it,f,Vt="Objective: Beat average player on colonist.io",ot,v,Xt="Building the model",at,x,Zt='On the outset, I wanted to do most things from scratch meaning I decided to not use a framework or any existing repos. I wanted to learn the basics of ML/RL and I got a lot of inspiration from the <a href="https://www.kaggle.com/code/scaomath/simple-neural-network-for-mnist-numpy-from-scratch" rel="nofollow">toy mnist problem with numpy</a> and <a href="https://karpathy.github.io/2016/05/31/rl/" rel="nofollow">Karpathy’s pong writeup</a>.',st,C,$t="The model I wanted to build is a basic one. For input game state, predict a suitable action.",rt,w,te="Inputs",ht,c,ee="<li>Game state<ul><li>Players resources</li> <li>Players dev cards</li> <li>Board state</li></ul></li>",ut,y,le="Prediction:",pt,b,ne="<li>Action to perform</li>",dt,g,ie="To model multiple actions per turn, we predict until the model sets pass. This assumes that each action is the same regardless of if it is part of a chain of actions or not.",mt,_,oe="Example: Turn",ft,T,ae="<li>Action 1</li> <li>Action 2</li> <li>Action 3</li> <li>until… Pass</li>",vt,H,se="To start off with and not wanting to scale up training a lot, I focused on a 2-player rendition of the game. It is simpler letting me focus on training to win.",xt,P,re="Metrics",Ct,k,he=`The main metric I looked at is <strong>turns to win</strong>. This helps ensure that they get generically better and better. The other are standard ML metrics <strong>avg loss</strong> (indicating how close action vs prediction) and <strong>avg rewards</strong> (the number of rewards for each player).
The expectation is that <strong>turns to win</strong> trends low, <strong>avg loss</strong> trends low and <strong>avg rewards</strong> tampers to a steady level.`,wt,I,ue="Reward allocation and the law of large numbers",ct,L,pe=`<strong>Reward allocation</strong> is a tricky business and I struggled in finding a good one. It’s essentially coming up with an intuition and then trying to build for it. For instance, the earliest edition involved only rewarding winners. This is basically running experiments!
The main difficulty comes from which action to reward. In this simple example below:`,yt,M,de="A → B → C → D → E → Win",bt,A,me="… it’s unclear how much reward to allocate to each of the actions. The best reward is to reward for winning and let the model learn which actions are important. However, the longer the list of actions, the more fuzzy the allocation gets. In theory, based on the law of large numbers, it doesn’t matter (too much) but not having intermediate rewards drastically lower training speed. However, on the flip-side, overengineering the rewards would put too much human intuition to it and make it work via defined rules rather than learned rules.",gt,q,fe="Rewarding winners",_t,j,ve=`The thought was the as you reward the winner’s actions, it will keep competing until it becomes the best.
Here shows the metrics:`,Tt,z,xe="Rewarding winners, Penalizing losers",Ht,R,Ce="Other than that, we could also penalize losers. This will speed up",Pt,B,we="Rewarding Settlements and Cities",kt,O,ce="Settlements and cities are pretty useful. Let’s reward them",It,S,ye="Rewarding points",Lt,F,be="A limitation of the settlements and cities approach is that it does not reward you if you get dev cards and or longest roads etc.",Mt,U,ge="Rewarding points could be a good middle ground.",At,E,_e="Training by evolution",qt,G,Te="An alternative to training on each player winning and losing is to train on the average winner over a batch. Although this could be slower (1/2 of rewards), it should be more accurate on which side is better, giving a better gradient.",jt,N,He="Action space rebalancing",zt,W,Pe="One thing that I noticed while training is that it overprioritised non-passing. A simple look at the action space and it’s became clear that because most actions have more than one action, passing is rarely randomly chosen. Rebalancing by a factor of the available actions looks to be a good idea. The idea here is to make passing initially the most possible action and let it slowly fall down.",Rt,D,ke="Building the colonist.io controller",Bt,J,Ie="The colonist.io controller was much more difficult and time consuming than I thought. Because of a lack of apis to get the state of the game. The difficulty increases because colonist.io uses a html canvas to display the game. This means it’s not possible to search for elements. What I ended up having to building an image matcher in order to get the state of the game. This includes mapping each of the little settlements and city icons and use cv2 to find where it matches the most. Here is the list of all little images.",Ot,K,Le="[]",St,Y,Me="There are essenetially many timeouts added to wait out animations. The images do not always match 100% making it also a bit unreliable.",Ft,Q,Ae="The canvas itself is also not the only piece of the information puzzle. The messages also need to be parsed inorder to provide additional information on whether dev cards are, what was rolled and what resources was gained.",Ut,V,qe="Combining the two",Et,X,je="Using the state gathered by the controller, I pass it to our latest and greatest model to battle it out with the colonist.io AI.",Gt,Z,ze="Here’s a short video of this.",Nt,$,Re="Over 100 games… we won…";return{c(){h=i("h1"),h.textContent=Dt,tt=s(),u=i("p"),u.innerHTML=Jt,et=s(),p=i("p"),p.textContent=Kt,lt=s(),d=i("ol"),d.innerHTML=Yt,nt=s(),m=i("p"),m.innerHTML=Qt,it=s(),f=i("p"),f.textContent=Vt,ot=s(),v=i("h2"),v.textContent=Xt,at=s(),x=i("p"),x.innerHTML=Zt,st=s(),C=i("p"),C.textContent=$t,rt=s(),w=i("p"),w.textContent=te,ht=s(),c=i("ul"),c.innerHTML=ee,ut=s(),y=i("p"),y.textContent=le,pt=s(),b=i("ul"),b.innerHTML=ne,dt=s(),g=i("p"),g.textContent=ie,mt=s(),_=i("p"),_.textContent=oe,ft=s(),T=i("ul"),T.innerHTML=ae,vt=s(),H=i("p"),H.textContent=se,xt=s(),P=i("h2"),P.textContent=re,Ct=s(),k=i("p"),k.innerHTML=he,wt=s(),I=i("h3"),I.textContent=ue,ct=s(),L=i("p"),L.innerHTML=pe,yt=s(),M=i("p"),M.textContent=de,bt=s(),A=i("p"),A.textContent=me,gt=s(),q=i("h4"),q.textContent=fe,_t=s(),j=i("p"),j.textContent=ve,Tt=s(),z=i("h4"),z.textContent=xe,Ht=s(),R=i("p"),R.textContent=Ce,Pt=s(),B=i("h4"),B.textContent=we,kt=s(),O=i("p"),O.textContent=ce,It=s(),S=i("h4"),S.textContent=ye,Lt=s(),F=i("p"),F.textContent=be,Mt=s(),U=i("p"),U.textContent=ge,At=s(),E=i("h4"),E.textContent=_e,qt=s(),G=i("p"),G.textContent=Te,jt=s(),N=i("h4"),N.textContent=He,zt=s(),W=i("p"),W.textContent=Pe,Rt=s(),D=i("h2"),D.textContent=ke,Bt=s(),J=i("p"),J.textContent=Ie,Ot=s(),K=i("p"),K.textContent=Le,St=s(),Y=i("p"),Y.textContent=Me,Ft=s(),Q=i("p"),Q.textContent=Ae,Ut=s(),V=i("h3"),V.textContent=qe,Et=s(),X=i("p"),X.textContent=je,Gt=s(),Z=i("p"),Z.textContent=ze,Nt=s(),$=i("p"),$.textContent=Re},l(t){h=o(t,"H1",{"data-svelte-h":!0}),a(h)!=="svelte-1sga3"&&(h.textContent=Dt),tt=r(t),u=o(t,"P",{"data-svelte-h":!0}),a(u)!=="svelte-1bsdffq"&&(u.innerHTML=Jt),et=r(t),p=o(t,"P",{"data-svelte-h":!0}),a(p)!=="svelte-9pqyyz"&&(p.textContent=Kt),lt=r(t),d=o(t,"OL",{"data-svelte-h":!0}),a(d)!=="svelte-8jfke"&&(d.innerHTML=Yt),nt=r(t),m=o(t,"P",{"data-svelte-h":!0}),a(m)!=="svelte-40e95f"&&(m.innerHTML=Qt),it=r(t),f=o(t,"P",{"data-svelte-h":!0}),a(f)!=="svelte-13xu26x"&&(f.textContent=Vt),ot=r(t),v=o(t,"H2",{"data-svelte-h":!0}),a(v)!=="svelte-1fb4wts"&&(v.textContent=Xt),at=r(t),x=o(t,"P",{"data-svelte-h":!0}),a(x)!=="svelte-fvt5e1"&&(x.innerHTML=Zt),st=r(t),C=o(t,"P",{"data-svelte-h":!0}),a(C)!=="svelte-83iqor"&&(C.textContent=$t),rt=r(t),w=o(t,"P",{"data-svelte-h":!0}),a(w)!=="svelte-f4xss3"&&(w.textContent=te),ht=r(t),c=o(t,"UL",{"data-svelte-h":!0}),a(c)!=="svelte-12ac24k"&&(c.innerHTML=ee),ut=r(t),y=o(t,"P",{"data-svelte-h":!0}),a(y)!=="svelte-1nopgm9"&&(y.textContent=le),pt=r(t),b=o(t,"UL",{"data-svelte-h":!0}),a(b)!=="svelte-eu4zyn"&&(b.innerHTML=ne),dt=r(t),g=o(t,"P",{"data-svelte-h":!0}),a(g)!=="svelte-a94typ"&&(g.textContent=ie),mt=r(t),_=o(t,"P",{"data-svelte-h":!0}),a(_)!=="svelte-8vxacn"&&(_.textContent=oe),ft=r(t),T=o(t,"UL",{"data-svelte-h":!0}),a(T)!=="svelte-qoa2av"&&(T.innerHTML=ae),vt=r(t),H=o(t,"P",{"data-svelte-h":!0}),a(H)!=="svelte-1rmwk5o"&&(H.textContent=se),xt=r(t),P=o(t,"H2",{"data-svelte-h":!0}),a(P)!=="svelte-152aplx"&&(P.textContent=re),Ct=r(t),k=o(t,"P",{"data-svelte-h":!0}),a(k)!=="svelte-nlgiw7"&&(k.innerHTML=he),wt=r(t),I=o(t,"H3",{"data-svelte-h":!0}),a(I)!=="svelte-4dgk93"&&(I.textContent=ue),ct=r(t),L=o(t,"P",{"data-svelte-h":!0}),a(L)!=="svelte-e2bwqz"&&(L.innerHTML=pe),yt=r(t),M=o(t,"P",{"data-svelte-h":!0}),a(M)!=="svelte-emdpmv"&&(M.textContent=de),bt=r(t),A=o(t,"P",{"data-svelte-h":!0}),a(A)!=="svelte-i8iq3f"&&(A.textContent=me),gt=r(t),q=o(t,"H4",{"data-svelte-h":!0}),a(q)!=="svelte-jlonrx"&&(q.textContent=fe),_t=r(t),j=o(t,"P",{"data-svelte-h":!0}),a(j)!=="svelte-1swfj91"&&(j.textContent=ve),Tt=r(t),z=o(t,"H4",{"data-svelte-h":!0}),a(z)!=="svelte-1ou8k3e"&&(z.textContent=xe),Ht=r(t),R=o(t,"P",{"data-svelte-h":!0}),a(R)!=="svelte-1kjijqm"&&(R.textContent=Ce),Pt=r(t),B=o(t,"H4",{"data-svelte-h":!0}),a(B)!=="svelte-1e74uz1"&&(B.textContent=we),kt=r(t),O=o(t,"P",{"data-svelte-h":!0}),a(O)!=="svelte-1ncrynk"&&(O.textContent=ce),It=r(t),S=o(t,"H4",{"data-svelte-h":!0}),a(S)!=="svelte-p6q0xq"&&(S.textContent=ye),Lt=r(t),F=o(t,"P",{"data-svelte-h":!0}),a(F)!=="svelte-1ezer4u"&&(F.textContent=be),Mt=r(t),U=o(t,"P",{"data-svelte-h":!0}),a(U)!=="svelte-fn9lcc"&&(U.textContent=ge),At=r(t),E=o(t,"H4",{"data-svelte-h":!0}),a(E)!=="svelte-1niq3xs"&&(E.textContent=_e),qt=r(t),G=o(t,"P",{"data-svelte-h":!0}),a(G)!=="svelte-miszf9"&&(G.textContent=Te),jt=r(t),N=o(t,"H4",{"data-svelte-h":!0}),a(N)!=="svelte-b6sbqq"&&(N.textContent=He),zt=r(t),W=o(t,"P",{"data-svelte-h":!0}),a(W)!=="svelte-1t3ou9h"&&(W.textContent=Pe),Rt=r(t),D=o(t,"H2",{"data-svelte-h":!0}),a(D)!=="svelte-j321vg"&&(D.textContent=ke),Bt=r(t),J=o(t,"P",{"data-svelte-h":!0}),a(J)!=="svelte-1oizk1m"&&(J.textContent=Ie),Ot=r(t),K=o(t,"P",{"data-svelte-h":!0}),a(K)!=="svelte-8q3uq8"&&(K.textContent=Le),St=r(t),Y=o(t,"P",{"data-svelte-h":!0}),a(Y)!=="svelte-k52p0j"&&(Y.textContent=Me),Ft=r(t),Q=o(t,"P",{"data-svelte-h":!0}),a(Q)!=="svelte-1w687bo"&&(Q.textContent=Ae),Ut=r(t),V=o(t,"H3",{"data-svelte-h":!0}),a(V)!=="svelte-zflkhf"&&(V.textContent=qe),Et=r(t),X=o(t,"P",{"data-svelte-h":!0}),a(X)!=="svelte-g6jzlq"&&(X.textContent=je),Gt=r(t),Z=o(t,"P",{"data-svelte-h":!0}),a(Z)!=="svelte-5r8q5f"&&(Z.textContent=ze),Nt=r(t),$=o(t,"P",{"data-svelte-h":!0}),a($)!=="svelte-tj1f3y"&&($.textContent=Re)},m(t,e){l(t,h,e),l(t,tt,e),l(t,u,e),l(t,et,e),l(t,p,e),l(t,lt,e),l(t,d,e),l(t,nt,e),l(t,m,e),l(t,it,e),l(t,f,e),l(t,ot,e),l(t,v,e),l(t,at,e),l(t,x,e),l(t,st,e),l(t,C,e),l(t,rt,e),l(t,w,e),l(t,ht,e),l(t,c,e),l(t,ut,e),l(t,y,e),l(t,pt,e),l(t,b,e),l(t,dt,e),l(t,g,e),l(t,mt,e),l(t,_,e),l(t,ft,e),l(t,T,e),l(t,vt,e),l(t,H,e),l(t,xt,e),l(t,P,e),l(t,Ct,e),l(t,k,e),l(t,wt,e),l(t,I,e),l(t,ct,e),l(t,L,e),l(t,yt,e),l(t,M,e),l(t,bt,e),l(t,A,e),l(t,gt,e),l(t,q,e),l(t,_t,e),l(t,j,e),l(t,Tt,e),l(t,z,e),l(t,Ht,e),l(t,R,e),l(t,Pt,e),l(t,B,e),l(t,kt,e),l(t,O,e),l(t,It,e),l(t,S,e),l(t,Lt,e),l(t,F,e),l(t,Mt,e),l(t,U,e),l(t,At,e),l(t,E,e),l(t,qt,e),l(t,G,e),l(t,jt,e),l(t,N,e),l(t,zt,e),l(t,W,e),l(t,Rt,e),l(t,D,e),l(t,Bt,e),l(t,J,e),l(t,Ot,e),l(t,K,e),l(t,St,e),l(t,Y,e),l(t,Ft,e),l(t,Q,e),l(t,Ut,e),l(t,V,e),l(t,Et,e),l(t,X,e),l(t,Gt,e),l(t,Z,e),l(t,Nt,e),l(t,$,e)},p:Wt,i:Wt,o:Wt,d(t){t&&(n(h),n(tt),n(u),n(et),n(p),n(lt),n(d),n(nt),n(m),n(it),n(f),n(ot),n(v),n(at),n(x),n(st),n(C),n(rt),n(w),n(ht),n(c),n(ut),n(y),n(pt),n(b),n(dt),n(g),n(mt),n(_),n(ft),n(T),n(vt),n(H),n(xt),n(P),n(Ct),n(k),n(wt),n(I),n(ct),n(L),n(yt),n(M),n(bt),n(A),n(gt),n(q),n(_t),n(j),n(Tt),n(z),n(Ht),n(R),n(Pt),n(B),n(kt),n(O),n(It),n(S),n(Lt),n(F),n(Mt),n(U),n(At),n(E),n(qt),n(G),n(jt),n(N),n(zt),n(W),n(Rt),n(D),n(Bt),n(J),n(Ot),n(K),n(St),n(Y),n(Ft),n(Q),n(Ut),n(V),n(Et),n(X),n(Gt),n(Z),n(Nt),n($))}}}const Ne={title:"Alpha catan: AI from the ground up",description:"My journey on building an AI from scratch.",tags:"Project",last:"23-10-24"};class We extends Se{constructor(h){super(),Fe(this,h,null,Ue,Oe,{})}}export{We as default,Ne as metadata};
